{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21f4c447",
   "metadata": {},
   "outputs": [],
   "source": [
    "from NeuralNetwork import NNetwork\n",
    "from sklearn.datasets import fetch_openml\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "845629bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X_train, y_train, X_val, y_val, batch_size=32, learning_rate=0.01, epochs=10, verbose=1, reg_type=\"L1\"):\n",
    "    \"\"\"\n",
    "    Melatih model dengan parameter yang diberikan.\n",
    "\n",
    "    :param model: Objek dari NNetwork.\n",
    "    :param X_train: Data training (numpy array, shape: (num_samples, num_features))\n",
    "    :param y_train: Label training (numpy array, shape: (num_samples, num_classes))\n",
    "    :param X_val: Data validasi (numpy array, shape: (num_samples, num_features))\n",
    "    :param y_val: Label validasi (numpy array, shape: (num_samples, num_classes))\n",
    "    :param batch_size: Jumlah sampel per batch saat training.\n",
    "    :param learning_rate: Learning rate untuk gradient descent.\n",
    "    :param epochs: Jumlah epoch untuk training.\n",
    "    :param verbose: 0 = tanpa output, 1 = progress bar + training & validation loss.\n",
    "    :return: Dictionary berisi histori training loss & validation loss tiap epoch.\n",
    "    \"\"\"\n",
    "    history = {\"train_loss\": [], \"val_loss\": []}\n",
    "    num_samples = X_train.shape[0]\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        num_batches = num_samples // batch_size\n",
    "\n",
    "        batch_iterator = tqdm(range(num_batches), desc=f\"Epoch {epoch+1}/{epochs}\", disable=(verbose == 0))\n",
    "\n",
    "        for batch_idx in batch_iterator:\n",
    "            start_idx = batch_idx * batch_size\n",
    "            end_idx = start_idx + batch_size\n",
    "            X_batch, y_batch = X_train[start_idx:end_idx], y_train[start_idx:end_idx]\n",
    "\n",
    "            loss = model.backward_propagation(X_batch, y_batch, learning_rate, reg_type)\n",
    "            epoch_loss += loss\n",
    "\n",
    "            if verbose == 1:\n",
    "                batch_iterator.set_postfix(train_loss=loss)\n",
    "\n",
    "        train_loss = epoch_loss / num_batches\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "\n",
    "        val_preds = model.forward_propagation(X_val)\n",
    "        val_loss = np.mean((val_preds - y_val) ** 2)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "\n",
    "        if verbose == 1:\n",
    "            print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.5f}, Val Loss: {val_loss:.5f}\")\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "412916ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = fetch_openml(\"mnist_784\", version=1, return_X_y=True, as_frame=False)\n",
    "X= X / 255\n",
    "y = np.array(y).astype(int)  # Convert to integers\n",
    "num_classes = 10  # since labels are from 0 to 9\n",
    "y = np.eye(num_classes)[y]\n",
    "\n",
    "# X = X[:1000]\n",
    "# y = y[:1000]\n",
    "\n",
    "Xtrain = X[:5600]\n",
    "ytrain = y[:5600]\n",
    "Xval = X[5600:]\n",
    "yval = y[5600:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78d1cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn1 = NNetwork(4 , [784, 16, 16, 10], verbose=True)\n",
    "nn1.initialize_weights(method=\"normal\", mean=0, variance=0.1, seed=42, verbose=False)\n",
    "nn2 = NNetwork(4 , [784, 16, 16, 10], verbose=True)\n",
    "nn2.initialize_weights(method=\"normal\", mean=0, variance=0.1, seed=42, verbose=False)\n",
    "nn3 = NNetwork(4 , [784, 16, 16, 10], verbose=True)\n",
    "nn3.initialize_weights(method=\"normal\", mean=0, variance=0.1, seed=42, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fa4fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "history1 = train_model(nn1, Xtrain, ytrain, Xval, yval, batch_size=20, learning_rate=0.01, epochs=10, verbose=1, reg_type=\"None\")\n",
    "history2 = train_model(nn2, Xtrain, ytrain, Xval, yval, batch_size=20, learning_rate=0.01, epochs=10, verbose=1, reg_type=\"L1\")\n",
    "history3 = train_model(nn3, Xtrain, ytrain, Xval, yval, batch_size=20, learning_rate=0.01, epochs=10, verbose=1, reg_type=\"L2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00ca4164",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history, base_dir=\"hasil\"):\n",
    "    \"\"\"Plot training history and save to a dynamically created folder.\"\"\"\n",
    "    # Cari folder dengan angka berikutnya yang belum ada\n",
    "    i = 1\n",
    "    while os.path.exists(f\"{base_dir}/{i}\"):\n",
    "        i += 1\n",
    "\n",
    "    filename = f\"training_history_{i}.png\"\n",
    "    folder_path = f\"{base_dir}/{i}\"\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "    # Simpan plot ke file di folder tersebut\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history[\"train_loss\"], label=\"Training Loss\", marker=\"o\")\n",
    "    plt.plot(history[\"val_loss\"], label=\"Validation Loss\", marker=\"s\")\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(file_path)  # Simpan plot ke file\n",
    "    plt.close()  # Tutup plot untuk menghindari masalah GUI\n",
    "\n",
    "    # Simpan data verbose ke file teks\n",
    "    verbosename = f\"verbose_{i}.txt\"\n",
    "    verbose_file_path = os.path.join(folder_path, verbosename)\n",
    "    with open(verbose_file_path, \"w\") as f:\n",
    "        f.write(json.dumps(history, indent=4))\n",
    "\n",
    "    print(f\"Plot saved to: {file_path}\")\n",
    "    print(f\"Verbose data saved to: {verbose_file_path}\")\n",
    "    return folder_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffa1246",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(history1)\n",
    "plot_training_history(history2)\n",
    "plot_training_history(history3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ef67fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.10.11' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages."
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Normalisasi data\n",
    "from sklearn.datasets import fetch_openml\n",
    "X, y = fetch_openml(\"mnist_784\", version=1, return_X_y=True, as_frame=False)\n",
    "X = X / 255.0\n",
    "y = np.array(y).astype(int)\n",
    "\n",
    "# One-hot encoding untuk y\n",
    "num_classes = 10\n",
    "y_onehot = np.eye(num_classes)[y]\n",
    "\n",
    "# Split manual agar sesuai\n",
    "Xtrain = X[:5600]\n",
    "ytrain = y[:5600]\n",
    "Xval = X[5600:]\n",
    "yval = y[5600:]\n",
    "yval_onehot = y_onehot[5600:]\n",
    "\n",
    "def train_sklearn_model(Xtrain, ytrain, Xval, yval, alpha=0.0001, reg_type=\"l2\", epochs=10, verbose=1):\n",
    "    history = {\"train_loss\": [], \"val_loss\": []}\n",
    "\n",
    "    model = MLPClassifier(hidden_layer_sizes=(16, 16),\n",
    "                          activation='relu',\n",
    "                          solver='sgd',\n",
    "                          learning_rate_init=0.01,\n",
    "                          max_iter=1,\n",
    "                          warm_start=True,\n",
    "                          batch_size=20,\n",
    "                          alpha=alpha,\n",
    "                          verbose=0)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.fit(Xtrain, ytrain)\n",
    "\n",
    "        # Predict untuk log loss (menggunakan probabilitas)\n",
    "        train_proba = model.predict_proba(Xtrain)\n",
    "        val_proba = model.predict_proba(Xval)\n",
    "\n",
    "        train_loss = log_loss(ytrain, train_proba)\n",
    "        val_loss = log_loss(yval, val_proba)\n",
    "\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.5f}, Val Loss: {val_loss:.5f}\")\n",
    "\n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb5b0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_none, history_none = train_sklearn_model(Xtrain, ytrain, Xval, yval, alpha=0.0, reg_type=\"none\")\n",
    "model_l1, history_l1 = train_sklearn_model(Xtrain, ytrain, Xval, yval, alpha=0.001, reg_type=\"l1\")\n",
    "model_l2, history_l2 = train_sklearn_model(Xtrain, ytrain, Xval, yval, alpha=0.001, reg_type=\"l2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130cfa2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history_sklearn(history, base_dir=\"hasil_sklearn\"):\n",
    "    i = 1\n",
    "    while os.path.exists(f\"{base_dir}/{i}\"):\n",
    "        i += 1\n",
    "\n",
    "    filename = f\"training_history_{i}.png\"\n",
    "    folder_path = f\"{base_dir}/{i}\"\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history[\"train_loss\"], label=\"Training Loss\", marker=\"o\")\n",
    "    plt.plot(history[\"val_loss\"], label=\"Validation Loss\", marker=\"s\")\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Log Loss')\n",
    "    plt.title('Training and Validation Loss (sklearn MLPClassifier)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(file_path)\n",
    "    plt.close()\n",
    "\n",
    "    verbosename = f\"verbose_{i}.txt\"\n",
    "    verbose_file_path = os.path.join(folder_path, verbosename)\n",
    "    with open(verbose_file_path, \"w\") as f:\n",
    "        f.write(json.dumps(history, indent=4))\n",
    "\n",
    "    print(f\"Plot saved to: {file_path}\")\n",
    "    print(f\"Verbose data saved to: {verbose_file_path}\")\n",
    "    return folder_path\n",
    "\n",
    "plot_training_history_sklearn(history_none)\n",
    "plot_training_history_sklearn(history_l1)\n",
    "plot_training_history_sklearn(history_l2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
